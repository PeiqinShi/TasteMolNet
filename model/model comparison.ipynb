{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d764b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Improved XGBoost classifier for Sweet vs Tasteless\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "data = pd.read_excel('../data/Rojas.xlsx')\n",
    "df = data.dropna(subset=[\"SMILES\", \"class\"]).copy()\n",
    "\n",
    "# Convert labels: sweet = 1, tasteless = 0\n",
    "df[\"label\"] = df[\"class\"].apply(lambda x: 1 if str(x).strip().lower() == \"sweet\" else 0)\n",
    "\n",
    "# Morgan (Circular) fingerprint\n",
    "featurizer = dc.feat.CircularFingerprint(size=1024)\n",
    "X = featurizer.featurize(df[\"SMILES\"])\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "def calc_metrics_counts(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    Sn = tp / (tp + fn) if (tp + fn) > 0 else 0.0    # Sensitivity\n",
    "    Sp = tn / (tn + fp) if (tn + fp) > 0 else 0.0    # Specificity\n",
    "    NER = (Sn + Sp) / 2.0                            # Balanced accuracy\n",
    "    return {\"Sn\": Sn, \"Sp\": Sp, \"NER\": NER, \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn}\n",
    "\n",
    "\n",
    "# SMOTE + XGBoost\n",
    "smote = SMOTE(random_state=42)\n",
    "model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"smote\", smote),\n",
    "    (\"xgb\", model)\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [100, 200, 300],\n",
    "    \"xgb__max_depth\": [3, 4, 5],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"xgb__subsample\": [0.8, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n===== Best Parameters =====\")\n",
    "print(grid.best_params_)\n",
    "print(f\"Best CV balanced accuracy: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Train best model\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# ROC curve for threshold selection\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
    "\n",
    "# Predictions using custom threshold\n",
    "y_pred_train = (best_model.predict_proba(X_train)[:, 1] >= optimal_threshold).astype(int)\n",
    "y_pred_test = (best_model.predict_proba(X_test)[:, 1] >= optimal_threshold).astype(int)\n",
    "\n",
    "# Performance metrics\n",
    "m_train = calc_metrics_counts(y_train, y_pred_train)\n",
    "m_test = calc_metrics_counts(y_test, y_pred_test)\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"Sn\":   [m_train[\"Sn\"], m_test[\"Sn\"]],\n",
    "        \"Sp\":   [m_train[\"Sp\"], m_test[\"Sp\"]],\n",
    "        \"NER\":  [m_train[\"NER\"], m_test[\"NER\"]],\n",
    "        \"TP\":   [m_train[\"TP\"], m_test[\"TP\"]],\n",
    "        \"FP\":   [m_train[\"FP\"], m_test[\"FP\"]],\n",
    "        \"TN\":   [m_train[\"TN\"], m_test[\"TN\"]],\n",
    "        \"FN\":   [m_train[\"FN\"], m_test[\"FN\"]],\n",
    "    },\n",
    "    index=[\"Train\", \"Test\"]\n",
    ").round(2)\n",
    "\n",
    "print(\"\\n===== Train & Test Metrics =====\")\n",
    "print(df_metrics)\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\n===== Class Distribution =====\")\n",
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed5a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"Sn\":   [m_train[\"Sn\"], m_test[\"Sn\"]],\n",
    "        \"Sp\":   [m_train[\"Sp\"], m_test[\"Sp\"]],\n",
    "        \"NER\":  [m_train[\"NER\"], m_test[\"NER\"]],\n",
    "        \"TP\":   [m_train[\"TP\"], m_test[\"TP\"]],\n",
    "        \"FP\":   [m_train[\"FP\"], m_test[\"FP\"]],\n",
    "        \"TN\":   [m_train[\"TN\"], m_test[\"TN\"]],\n",
    "        \"FN\":   [m_train[\"FN\"], m_test[\"FN\"]],\n",
    "    },\n",
    "    index=[\"Train\", \"Test\"]\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Train & Test Metrics =====\")\n",
    "print(df_metrics)\n",
    "\n",
    "# 12. Class distribution information\n",
    "print(\"\\n===== Class Distribution =====\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8859b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 9 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 11 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 8 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 5 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 7 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 9 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 11 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 3 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[09:10:18] Explicit valence for atom # 8 N, 4, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ— æ•ˆçš„ SMILES æ¡ç›®ï¼š\n",
      "    number                               name  \\\n",
      "274    275                Denatonium chloride   \n",
      "275    276   Denatonium chloride derivative 1   \n",
      "276    277   Denatonium chloride derivative 2   \n",
      "277    278   Denatonium chloride derivative 3   \n",
      "278    279   Denatonium chloride derivative 4   \n",
      "279    280   Denatonium chloride derivative 5   \n",
      "280    281   Denatonium chloride derivative 6   \n",
      "281    282   Denatonium chloride derivative 7   \n",
      "282    283   Denatonium chloride derivative 8   \n",
      "283    284   Denatonium chloride derivative 9   \n",
      "284   285^  Denatonium chloride derivative 10   \n",
      "285    286  Denatonium chloride derivative 11   \n",
      "286   287^  Denatonium chloride derivative 12   \n",
      "287   288^  Denatonium chloride derivative 13   \n",
      "\n",
      "                                                SMILES   class reference  \\\n",
      "274        CC[N](Cc1ccc(cc1)Cl)(CC(=O)Nc1c(C)cccc1C)CC  Bitter       NaN   \n",
      "275                    ClC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "276                   BrCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "277                  ICCCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "278                ClCCCCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "279               ClCCCCCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "280             ClCCCCCCCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "281           ClCCCCCCCCCC[N](CC(=O)Nc1c(C)cccc1C)(C)C  Bitter       NaN   \n",
      "282          O=C(C[N](Cc1ccc(cc1)Cl)(C)C)Nc1c(C)cccc1C  Bitter       NaN   \n",
      "283         O=C(C[N](CCc1ccc(cc1)Cl)(C)C)Nc1c(C)cccc1C  Bitter       NaN   \n",
      "284        O=C(C[N](CCCc1ccc(cc1)Cl)(C)C)Nc1c(C)cccc1C  Bitter       NaN   \n",
      "285         CC[N](Cc1ccc(cc1)Cl)(CC(=O)Nc1c(C)cccc1C)C  Bitter       NaN   \n",
      "286       CCCC[N](Cc1ccc(cc1)Cl)(CC(=O)Nc1c(C)cccc1C)C  Bitter       NaN   \n",
      "287  Clc1ccc(cc1)C[N](Cc1ccccc1)(CC(=O)Nc1c(C)cccc1C)C  Bitter       NaN   \n",
      "\n",
      "     label  \n",
      "274      1  \n",
      "275      1  \n",
      "276      1  \n",
      "277      1  \n",
      "278      1  \n",
      "279      1  \n",
      "280      1  \n",
      "281      1  \n",
      "282      1  \n",
      "283      1  \n",
      "284      1  \n",
      "285      1  \n",
      "286      1  \n",
      "287      1  \n",
      "\n",
      "å·²åˆ é™¤æ— æ•ˆ SMILESï¼Œå½“å‰å‰©ä½™æ ·æœ¬æ•°ï¼š494\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "===== æœ€ä¼˜å‚æ•° =====\n",
      "{'xgb__colsample_bytree': 0.9, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__n_estimators': 200, 'xgb__subsample': 0.8}\n",
      "æœ€ä½³CV balanced accuracy: 0.758\n",
      "\n",
      "æœ€ä¼˜é˜ˆå€¼: 0.20\n",
      "\n",
      "===== è®­ç»ƒé›† & æµ‹è¯•é›†æŒ‡æ ‡ =====\n",
      "        Sn    Sp   NER  TP  FP   TN  FN\n",
      "Train  1.0  0.84  0.92  54  53  288   0\n",
      "Test   1.0  0.81  0.91  13  16   70   0\n",
      "\n",
      "===== æ ·æœ¬åˆ†å¸ƒ =====\n",
      "label\n",
      "0    427\n",
      "1     67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# XGBoost + MACCS model for Bitter vs Sweet\n",
    "# Handles class imbalance with SMOTE\n",
    "# Removes invalid SMILES automatically\n",
    "# ===========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from rdkit import Chem\n",
    "\n",
    "# 1ï¸âƒ£ Load data\n",
    "data = pd.read_excel('../data/Rojas.xlsx', sheet_name='Table 2S')\n",
    "df = data.dropna(subset=[\"SMILES\", \"class\"]).copy()\n",
    "\n",
    "# 2ï¸âƒ£ Label encoding: Bitter = 1, others = 0\n",
    "df[\"label\"] = df[\"class\"].apply(lambda x: 1 if str(x).strip().lower() == \"bitter\" else 0)\n",
    "\n",
    "# 3ï¸âƒ£ Validate SMILES strings\n",
    "def valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol is not None\n",
    "\n",
    "# Identify invalid SMILES\n",
    "invalid_smiles = df[~df[\"SMILES\"].apply(valid_smiles)]\n",
    "print(\"Invalid SMILES entries:\")\n",
    "print(invalid_smiles)\n",
    "\n",
    "# Remove invalid SMILES\n",
    "df = df[df[\"SMILES\"].apply(valid_smiles)].reset_index(drop=True)\n",
    "print(f\"\\nInvalid SMILES removed. Remaining sample size: {len(df)}\")\n",
    "\n",
    "# 4ï¸âƒ£ MACCS keys fingerprint\n",
    "featurizer = dc.feat.MACCSKeysFingerprint()\n",
    "X = featurizer.featurize(df[\"SMILES\"])\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# 5ï¸âƒ£ Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ Metric calculation\n",
    "def calc_metrics_counts(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    Sn = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    Sp = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    NER = (Sn + Sp) / 2.0\n",
    "    return {\"Sn\": Sn, \"Sp\": Sp, \"NER\": NER, \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn}\n",
    "\n",
    "# 7ï¸âƒ£ SMOTE + XGBoost pipeline\n",
    "smote = SMOTE(random_state=42)\n",
    "model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"smote\", smote),\n",
    "    (\"xgb\", model)\n",
    "])\n",
    "\n",
    "# 8ï¸âƒ£ Hyperparameter grid search\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [100, 200, 300],\n",
    "    \"xgb__max_depth\": [3, 4, 5],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"xgb__subsample\": [0.8, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n===== Best Parameters =====\")\n",
    "print(grid.best_params_)\n",
    "print(f\"Best CV balanced accuracy: {grid.best_score_:.3f}\")\n",
    "\n",
    "# 9ï¸âƒ£ Train the best model\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# ğŸ”Ÿ Determine optimal threshold (ROC-based)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
    "\n",
    "# â“« Predict with optimal threshold\n",
    "y_pred_train = (best_model.predict_proba(X_train)[:, 1] >= optimal_threshold).astype(int)\n",
    "y_pred_test  = (best_model.predict_proba(X_test)[:, 1]  >= optimal_threshold).astype(int)\n",
    "\n",
    "# â“¬ Performance metrics\n",
    "m_train = calc_metrics_counts(y_train, y_pred_train)\n",
    "m_test  = calc_metrics_counts(y_test,  y_pred_test)\n",
    "\n",
    "df_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"Sn\":   [m_train[\"Sn\"], m_test[\"Sn\"]],\n",
    "        \"Sp\":   [m_train[\"Sp\"], m_test[\"Sp\"]],\n",
    "        \"NER\":  [m_train[\"NER\"], m_test[\"NER\"]],\n",
    "        \"TP\":   [m_train[\"TP\"], m_test[\"TP\"]],\n",
    "        \"FP\":   [m_train[\"FP\"], m_test[\"FP\"]],\n",
    "        \"TN\":   [m_train[\"TN\"], m_test{\"TN\"]],\n",
    "        \"FN\":   [m_train[\"FN\"], m_test[\"FN\"]],\n",
    "    },\n",
    "    index=[\"Train\", \"Test\"]\n",
    ").round(2)\n",
    "\n",
    "print(\"\\n===== Train & Test Metrics =====\")\n",
    "print(df_metrics)\n",
    "\n",
    "# â“­ Class distribution\n",
    "print(\"\\n===== Class Distribution =====\")\n",
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdb5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ‡ç­¾åˆ†å¸ƒï¼š\n",
      "class\n",
      "B    685\n",
      "S    517\n",
      "Name: count, dtype: int64\n",
      "\n",
      "æ ‡ç­¾ç¼–ç ï¼š{'B': 0, 'S': 1}\n",
      "\n",
      "æœ‰æ•ˆæ•°æ®é‡ï¼š1202\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "===== æœ€ä¼˜å‚æ•° =====\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.8}\n",
      "\n",
      "===== è®­ç»ƒé›† & æµ‹è¯•é›†æŒ‡æ ‡ =====\n",
      "         Sn    Sp   NER   TP  FP   TN  FN\n",
      "Train  0.99  0.99  0.99  408   5  543   5\n",
      "Test   0.94  0.93  0.94   98   9  128   6\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# XGBoost + MACCS model (balanced dataset)\n",
    "# Output: Sn / Sp / NER\n",
    "# ===========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "from rdkit import Chem\n",
    "\n",
    "# 1ï¸âƒ£ Load dataset (modify this path)\n",
    "df = pd.read_csv(\"../data/Banerjee.csv\")   # <-- change your filename here\n",
    "\n",
    "# 2ï¸âƒ£ Remove invalid rows\n",
    "df = df.dropna(subset=[\"SMILES\", \"class\"]).copy()\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df[\"class\"].value_counts())\n",
    "\n",
    "# 3ï¸âƒ£ Label encoding (automatic binary conversion)\n",
    "classes = sorted(df[\"class\"].unique())\n",
    "mapping = {classes[0]: 0, classes[1]: 1}\n",
    "print(f\"\\nLabel encoding: {mapping}\")\n",
    "df[\"label\"] = df[\"class\"].map(mapping)\n",
    "\n",
    "# 4ï¸âƒ£ Check SMILES validity\n",
    "def valid_smiles(s):\n",
    "    return Chem.MolFromSmiles(s) is not None\n",
    "\n",
    "df = df[df[\"SMILES\"].apply(valid_smiles)].reset_index(drop=True)\n",
    "print(f\"\\nValid data count: {len(df)}\")\n",
    "\n",
    "# 5ï¸âƒ£ MACCS fingerprint features\n",
    "featurizer = dc.feat.MACCSKeysFingerprint()\n",
    "X = featurizer.featurize(df[\"SMILES\"])\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# 6ï¸âƒ£ Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 7ï¸âƒ£ Custom metrics\n",
    "def calc_metrics_counts(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    Sn = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    Sp = tn / (tn + fp) if tn + fp > 0 else 0.0\n",
    "    NER = (Sn + Sp) / 2.0\n",
    "    return Sn, Sp, NER, tp, fp, tn, fn\n",
    "\n",
    "# 8ï¸âƒ£ XGBoost hyperparameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n===== Best Parameters =====\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# 9ï¸âƒ£ Predictions (threshold = 0.5)\n",
    "y_pred_train = (best_model.predict_proba(X_train)[:, 1] >= 0.5).astype(int)\n",
    "y_pred_test = (best_model.predict_proba(X_test)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# ğŸ”Ÿ Evaluate metrics\n",
    "Sn_tr, Sp_tr, NER_tr, TP_tr, FP_tr, TN_tr, FN_tr = calc_metrics_counts(y_train, y_pred_train)\n",
    "Sn_te, Sp_te, NER_te, TP_te, FP_te, TN_te, FN_te = calc_metrics_counts(y_test, y_pred_test)\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Sn\": [Sn_tr, Sn_te],\n",
    "    \"Sp\": [Sp_tr, Sp_te],\n",
    "    \"NER\": [NER_tr, NER_te],\n",
    "    \"TP\": [TP_tr, TP_te],\n",
    "    \"FP\": [FP_tr, FP_te],\n",
    "    \"TN\": [TN_tr, TN_te],\n",
    "    \"FN\": [FN_tr, FN_te],\n",
    "}, index=[\"Train\", \"Test\"]).round(2)\n",
    "\n",
    "print(\"\\n===== Train & Test Metrics =====\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da03676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bittersweet",
   "language": "python",
   "name": "bittersweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
